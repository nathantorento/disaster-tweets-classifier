{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appnope==0.1.0 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
      "Collecting blis==0.4.1\n",
      "  Using cached blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0 MB)\n",
      "Collecting catalogue==1.0.0\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting certifi==2020.11.8\n",
      "  Using cached certifi-2020.11.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: chardet==3.0.4 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (3.0.4)\n",
      "Collecting cymem==2.0.4\n",
      "  Using cached cymem-2.0.4-cp37-cp37m-macosx_10_9_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: decorator==4.4.2 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.4.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en-core-web-sm==2.3.1 (from -r requirements.txt (line 9)) (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for en-core-web-sm==2.3.1 (from -r requirements.txt (line 9))\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing all dependencies required for the notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # 7613 rows, with 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target 1 refers to disaster tweet, 0 is not a disaster tweet\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 rows have no keywords\n",
      "2533 rows have no location\n",
      "0 rows have no text\n",
      "0 rows have no target\n"
     ]
    }
   ],
   "source": [
    "# checking for completeness of data\n",
    "print(f\"{np.sum(df['keyword'].isna())} rows have no keywords\")\n",
    "print(f\"{np.sum(df['location'].isna())} rows have no location\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no text\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "body%20bags              41\n",
       "sinking                  41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that some keywords are phrases, with '%20' as a space\n",
    "df['keyword'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                           104\n",
       "New York                       71\n",
       "United States                  50\n",
       "London                         45\n",
       "Canada                         29\n",
       "                             ... \n",
       "#partsunknown                   1\n",
       "AZ                              1\n",
       "Spring Grove, IL                1\n",
       "Somewhere in Jersey             1\n",
       "V-RP @OZRP_ ?MV, AU, R18+?      1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that there are some non-location locations, like 'World Wide!!' and 'a feminist, modernist hag.'\n",
    "df['location'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in ./venv/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.9/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.52.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/weiting/Code/disaster-tweets-classifier/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download spaCy model for American English\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying spaCy's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "2020\t\t2020\t\tFalse\n",
      "ca\t\tcan\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "get\t\tget\t\tTrue\n",
      "any\t\tany\t\tTrue\n",
      "worse\t\tbad\t\tFalse\n",
      "#\t\t#\t\tFalse\n",
      "ihate2020\t\tihate2020\t\tFalse\n",
      "@bestfriend\t\t@bestfriend\t\tFalse\n",
      "https://t.co\t\thttps://t.co\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Let's see what spaCy does with numbers, contractions, #hashtags, @mentions and URLs\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend https://t.co\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "2020\t\t2020\t\tFalse\n",
      "ca\t\tcan\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "get\t\tget\t\tTrue\n",
      "any\t\tany\t\tTrue\n",
      "worse\t\tbad\t\tFalse\n",
      "#ihate2020\t\t#ihate2020\t\tFalse\n",
      "@bestfriend\t\t@bestfriend\t\tFalse\n",
      "https://t.co\t\thttps://t.co\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Contractions are split into lemmas\n",
    "# Numbers are their own features\n",
    "# @mentions are maintained as a token\n",
    "# We want to also keep #hashtags as a token, so we will modify the spaCy model's token_match\n",
    "\n",
    "import re \n",
    "\n",
    "# Retrieve the default token-matching regex pattern\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n",
    "\n",
    "# Add #hashtag pattern\n",
    "re_token_match = f\"({re_token_match}|#\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Now let's try again\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend https://t.co\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "our\t\t-PRON-\t\tTrue\n",
      "deeds\t\tdeed\t\tFalse\n",
      "are\t\tbe\t\tTrue\n",
      "the\t\tthe\t\tTrue\n",
      "reason\t\treason\t\tFalse\n",
      "of\t\tof\t\tTrue\n",
      "this\t\tthis\t\tTrue\n",
      "#earthquake\t\t#earthquake\t\tFalse\n",
      "may\t\tmay\t\tTrue\n",
      "allah\t\tallah\t\tFalse\n",
      "forgive\t\tforgive\t\tFalse\n",
      "us\t\t-PRON-\t\tTrue\n",
      "all\t\tall\t\tTrue\n",
      "@mention\t\t@mention\t\tFalse\n",
      "#hashtag\t\t#hashtag\t\tFalse\n",
      "http://t.co/test\t\thttp://t.co/test\t\tFalse\n",
      "<class 'str'>\n",
      "Bag of words for the tweet: {'#': 2, '@': 1, 'URL': 1, '-PRON-': 2, 'deed': 1, 'be': 1, 'the': 1, 'reason': 1, 'of': 1, 'this': 1, '#earthquake': 1, 'may': 1, 'allah': 1, 'forgive': 1, 'all': 1, '@mention': 1, '#hashtag': 1, 'http://t.co/test': 1}\n"
     ]
    }
   ],
   "source": [
    "# Features is a set of all lemmas (words) encountered thus far, add hashtags, mentions and URLs to track the number of each respectively\n",
    "features = set({'#','@','URL'})\n",
    "\n",
    "# Now let's process an original tweet with our modified spaCy model\n",
    "s = df.loc[1,'text']\n",
    "print(f\"Original tweet: {s}\")\n",
    "\n",
    "# Modifying the tweet to include mentions, hashtags and a URL\n",
    "s += ' @mention #hashtag http://t.co/test'\n",
    "\n",
    "# To lowercase\n",
    "s = s.lower()\n",
    "\n",
    "# Creating a doc with spaCy\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "# Union between lemmas and our features set\n",
    "features |= set(lemmas)\n",
    "\n",
    "# Constructing a bag of words for the tweet\n",
    "freq = {'#':0,'@':0,'URL':0}\n",
    "for word in lemmas:\n",
    "    freq[str(word)] = 0\n",
    "for token in doc: \n",
    "    if '#' in str(token): freq['#'] += 1 # Count number of hashtags, regardless of hashtag\n",
    "    if '@' in str(token): freq['@'] += 1 # Count number of mentions, regardless of mention\n",
    "    if 'http://' in str(token): freq['URL'] += 1 # Count number of URLs, regardless of URL\n",
    "    freq[str(token.lemma_)] += 1\n",
    "print(type(str(token)))\n",
    "print(f\"Bag of words for the tweet: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've preprocessed a single tweet, we can create a pre-process function for each tweet\n",
    "def preprocess(s, nlp, features):\n",
    "    \"\"\"\n",
    "    Given string s, spaCy model nlp, and set features (lemmas encountered),\n",
    "    pre-process s and return updated features and bag-of-words representation dict freq\n",
    "    - changes s to lower-case\n",
    "    - tokenize s using nlp to create a doc\n",
    "    - update features with lemmas encountered in s\n",
    "    - create bag-of-words representation in dict type freq, including counts for hashtags, mentions and URLs\n",
    "    \"\"\"\n",
    "\n",
    "    # To lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Creating a doc with spaCy\n",
    "    doc = nlp(s)\n",
    "\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Union between lemmas and our features set\n",
    "    features |= set(lemmas)\n",
    "\n",
    "    # Constructing a bag of words for the tweet\n",
    "    freq = {'#':0,'@':0,'URL':0}\n",
    "    for word in lemmas:\n",
    "        freq[str(word)] = 0\n",
    "    for token in doc: \n",
    "        if '#' in str(token): freq['#'] += 1 # Count number of hashtags, regardless of hashtag\n",
    "        if '@' in str(token): freq['@'] += 1 # Count number of mentions, regardless of mention\n",
    "        if 'http://' in str(token): freq['URL'] += 1 # Count number of URLs, regardless of URL\n",
    "        freq[str(token.lemma_)] += 1\n",
    "        \n",
    "    return features, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = df # Duplicate for preprocessing\n",
    "features = set({'#','@','URL'}) # Using set feature to contain all words (lemmas) seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_array = [] # Array bow_array of bow representations for each tweet; bow_array[i] is the bow representation for tweet id (i+1)\n",
    "for i in range(len(preprocess_df)):\n",
    "    features, freq = preprocess(preprocess_df.iloc[i]['text'],nlp,features)\n",
    "    bow_array.append(freq)\n",
    "len(bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for bag of words representation for each tweet\n",
    "bow = pd.DataFrame('0', columns=features,index=range(1,len(preprocess_df)+1))\n",
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bow[i] with bag-of-words freq of the tweet id (i+1)\n",
    "for i in range(len(preprocess_df)):\n",
    "    freq = bow_array[i]\n",
    "    for f in freq:\n",
    "        bow.loc[i+1,f]=freq[f]\n",
    "\n",
    "# Join bag-of-words representation to train dataframe\n",
    "# Append _data suffix to 'keyword','location','text','target' for features that are not lemma tokens\n",
    "preprocess_df = preprocess_df.join(bow,lsuffix='_data')\n",
    "\n",
    "# Saving bag-of-words representation for collaborators\n",
    "preprocess_df.to_csv(\"data/train_preprocessed.csv\",index=True,index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location_data</th>\n",
       "      <th>text_data</th>\n",
       "      <th>target_data</th>\n",
       "      <th>drinking</th>\n",
       "      <th>courtney</th>\n",
       "      <th>@blazerfan</th>\n",
       "      <th>#speakingfromexperience</th>\n",
       "      <th>@cortezera</th>\n",
       "      <th>faculty</th>\n",
       "      <th>...</th>\n",
       "      <th>http://t.co/ixw2cutk1c</th>\n",
       "      <th>#oklahoma</th>\n",
       "      <th>http://t.co/s4sicmyrmh</th>\n",
       "      <th>airplane</th>\n",
       "      <th>95</th>\n",
       "      <th>aaarrrgghhh</th>\n",
       "      <th>https://t.co/cvkqigr1az</th>\n",
       "      <th>screams~</th>\n",
       "      <th>#silver</th>\n",
       "      <th>http://t.co/cubc0nq6fd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location_data                                          text_data  \\\n",
       "id                                                                            \n",
       "1      NaN           NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4      NaN           NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5      NaN           NaN  All residents asked to 'shelter in place' are ...   \n",
       "6      NaN           NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7      NaN           NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "    target_data drinking courtney @blazerfan #speakingfromexperience  \\\n",
       "id                                                                     \n",
       "1             1        0        0          0                       0   \n",
       "4             1        0        0          0                       0   \n",
       "5             1        0        0          0                       0   \n",
       "6             1        0        0          0                       0   \n",
       "7             1        0        0          0                       0   \n",
       "\n",
       "   @cortezera faculty  ... http://t.co/ixw2cutk1c #oklahoma  \\\n",
       "id                     ...                                    \n",
       "1           0       0  ...                      0         0   \n",
       "4           0       0  ...                      0         0   \n",
       "5           0       0  ...                      0         0   \n",
       "6           0       0  ...                      0         0   \n",
       "7           0       0  ...                      0         0   \n",
       "\n",
       "   http://t.co/s4sicmyrmh airplane 95 aaarrrgghhh https://t.co/cvkqigr1az  \\\n",
       "id                                                                          \n",
       "1                       0        0  0           0                       0   \n",
       "4                       0        0  0           0                       0   \n",
       "5                       0        0  0           0                       0   \n",
       "6                       0        0  0           0                       0   \n",
       "7                       0        0  0           0                       0   \n",
       "\n",
       "   screams~ #silver http://t.co/cubc0nq6fd  \n",
       "id                                          \n",
       "1         0       0                      0  \n",
       "4         0       0                      0  \n",
       "5         0       0                      0  \n",
       "6         0       0                      0  \n",
       "7         0       0                      0  \n",
       "\n",
       "[5 rows x 21330 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 21330) (762, 21330)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratify=y creates a balanced validation set\n",
    "y = preprocess_df['target_data']\n",
    "\n",
    "df_train, df_val = train_test_split(preprocess_df, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "# Saving csv files for collaborators\n",
    "df_train.to_csv(\"data/train_preprocessed_split.csv\",index=True)\n",
    "df_val.to_csv(\"data/val_preprocessed_split.csv\",index=True)\n",
    "\n",
    "print(df_train.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratio of target=1 to target=0 tweets in:\n",
      " \n",
      "Original data set = 0.7533394748963611,\n",
      "\n",
      "Training data set = 0.7535193242897363,\n",
      "\n",
      "Validation data set = 0.7517241379310344\n"
     ]
    }
   ],
   "source": [
    "# Checking balance\n",
    "print(f\"\"\"\n",
    "Ratio of target=1 to target=0 tweets in:\\n \n",
    "Original data set = {np.sum(preprocess_df['target_data']==1)/np.sum(preprocess_df['target_data']==0)},\\n\n",
    "Training data set = {np.sum(df_train['target_data']==1)/np.sum(df_train['target_data']==0)},\\n\n",
    "Validation data set = {np.sum(df_val['target_data']==1)/np.sum(df_val['target_data']==0)}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdsm",
   "language": "python",
   "name": "pdsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
