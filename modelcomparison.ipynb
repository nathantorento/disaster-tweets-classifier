{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Overview\n",
    "This paper will be based on the Kaggle competition Real or Not? NLP with Disaster Tweets (https://www.kaggle.com/c/nlp-getting-started). It's an introductory challenge to serve as practice for Natural Language Processing with focus on Text Classification. The competition creators gathered 10875  tweets that are reporting an emergency or some man-made/natural disaster â€“ the selection process is left unspecified. \n",
    "\n",
    "\n",
    "### Dataset\n",
    "There are three provided files:\n",
    "- train.csv - the training set\n",
    "- test.csv - the test set\n",
    "- sample_submission.csv - the framework for official competition submissions\n",
    "\n",
    "The training dataset contains these columns:\n",
    "- id: a unique numeric identifier for each tweet\n",
    "- text: the actual content in the tweet\n",
    "- keyword: keywords from the tweet manually selected by the competition creators (may be blank)\n",
    "- location: the location the tweet was sent from (may be blank)\n",
    "- target: values are hand classified by creators as either 0 for non-disaster or 1 for disaster\n",
    "\n",
    "The test set is similar to the training data set except for not having a target column.\n",
    "The sample_submission contains the id and a target column for all the tweets that competitors will have to populate with their proposed models.\n",
    "\n",
    "### Goal\n",
    "The challenge of the competition is to create a model that can most accurately predict the test set and achieve a high F1 Score (read more at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "For our team, however, we're hoping to use this challenge as our final academic opportunity to apply as many of the skills we've learned in our Practical Data Science Tutorial while exploring additional Data Science fields and methods we're interested in. \n",
    "\n",
    "Thus, we decided to create an NLP pipeline for this challenge, split it into four parts, and self-allocate as you will see in the Team Roles section below.\n",
    "\n",
    "Important Note: We all worked on different sections of a pipeline, and those sections naturally vary in magnitude of required effort. Thus, while we consolidated efforts to create output that fed or flowed into each other's section, we each individually explored into aspects of the data or task that were not necessary for the pipeline.\n",
    "\n",
    "\n",
    "### Team Roles\n",
    "- Wei-Ting Yap: data exploration and pre-processing\n",
    "- Emma Stiefel: data pre-processing and completion of data\n",
    "- Nathan Torento (me): model creation and comparison\n",
    "- Henry Biko: model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and pre-processing\n",
    "\n",
    "To explore and pre-process the training dataset, Wei-Ting chose to use the open-source NLP library for Python called spaCy. It is a level up from the NLTK library in that it is object oriented rather than string processing, computationally faster, and provides more customizability.\n",
    "\n",
    "There are an infinite permutation of implementations to preprocess text. Wei-Ting chose to pre-process the tweets such that for every tweet, she would take these steps:\n",
    "- identify all # (hashtags)\n",
    "- identify all @ (Twitter account mentions in-tweet)\n",
    "- identify all URLs\n",
    "- lowercase\n",
    "- tokenize\n",
    "- lemmatize\n",
    "- remove stopwords\n",
    "- bag of words\n",
    "\n",
    "These are all standard NLP pre-processing steps, except Wei-Ting also considers the format and context of tweets by distinguishing the non-plain text that are likely to appear as well. Thus, she creates a Bag of Words for each row that counts the instances of text starting with #, text starting with @, URLs, and all lemmatize non-stop words.\n",
    "\n",
    "#### For good measure, I would also remove emojis by searching and deleting all instances of their unicodes with regex.\n",
    "\n",
    "Her final output, however, implements \"count vectorization\" - an extension on the Bag of Words (BoW) concept. \n",
    "\n",
    "### Count Vectorization\n",
    "After creating a bag of words for every tweet, every \"word\" (or string) that has ever appeared is now added as a separate column of the dataset. Every row then places the number of times the strings in its lemmatized, non-stopword, bag of words appear for each column. This is a lengthy process to run on nearly 6000 tweets as it results in >20000 columns and a >200 mb dataset, so Wei-Ting saved the pre-processed .csv for easy access.\n",
    "\n",
    "#### My only qualm with this manual count vectorization is that it scales the dataset dimensions and size exponentially, making it more computationally costly and timely to load and use the dataset. It took me nearly 10 minutes to load the training data each runtime. Thus, for the model section later, I slightly modify Wei-Ting's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing and completion of data\n",
    "\n",
    "Emma decided to take on the challenge of completely filling in the \"location\" section, as well as further spell-checking the dataset. \n",
    "\n",
    "### Location\n",
    "Emma used the Mordecai library, which uses spaCy, Geonames, and the TensorFlow interact keras, to \"Geoparse\" the tweets. The library is often used to parse through text, identify information about geographical locations, then return these locations' coordinates.\n",
    "\n",
    "#### Although not necessary for the tweet disaster classification pipeline, in real life, location is crucial information for emergency response and disaster response planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and comparison\n",
    "\n",
    "Nathan (me, writing in 3rd Person)\n",
    "\n",
    "From all the possible classification models, Nathan chose to use the ones he had previous experience with:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Support Vector Machine\n",
    "- Multinomial Naive Bayes Model\n",
    "\n",
    "For each one, he implemented a simple implementation without any parameter specification.\n",
    "\n",
    "He also implemented a deep neural network, for which he justifies his decisions for every layer.\n",
    "\n",
    "For every model, he then compared the train accuracy, test accuracy, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---[Adjusted] Pre-processing code from Wei-Ting---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Creating a new column in the train data containing the preprocessed text later\n",
    "train.insert(loc=4, column=\"cleaned_text\", value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "  cleaned_text  target  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "import en_core_web_md\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stopwords.add(\"-PRON-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = set({'#','@','URL'}) # Using set feature to contain all words (lemmas) seen\n",
    "\n",
    "# Now that we've preprocessed a single tweet, we can create a pre-process function for each tweet\n",
    "def preprocess(s):\n",
    "    \"\"\"\n",
    "    Given string s, spaCy model nlp, and set features (lemmas encountered),\n",
    "    pre-process s and return updated features and bag-of-words representation dict freq\n",
    "    - changes s to lower-case\n",
    "    - remove punctuationo\n",
    "    - tokenize s using nlp to create a doc\n",
    "    - update features with lemmas encountered in s\n",
    "    \"\"\"\n",
    "    # Split\n",
    "    s = s.split()\n",
    "    \n",
    "    # Lowercase\n",
    "    s = [word.lower() for word in s]\n",
    "    \n",
    "    # Remove non-letters\n",
    "    s = [re.sub(r'[^a-zA-Z]', '', word) for word in s]\n",
    "    \n",
    "    # Remove non-letters\n",
    "    s = [re.sub(r'[^\\w\\s]', '', word) for word in s]\n",
    "    \n",
    "    # spaCy conversion for lemmatization\n",
    "    s = \" \".join(s)\n",
    "    s = nlp(s)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmas = [token.lemma_ for token in s]\n",
    "        \n",
    "    # Remove stopwords including -PRON\n",
    "    # Note: spaCy lemmatizes all pronouns as -PRON-\n",
    "    cleaned_lemmas = [word for word in lemmas if not word in stopwords]\n",
    "    \n",
    "    cleaned_lemmas = \" \".join(cleaned_lemmas)\n",
    "    \n",
    "    return cleaned_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>send photo ruby alaska smoke wildfire pour school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                        cleaned_text  target  \n",
       "0               deed reason earthquake allah forgive       1  \n",
       "1              forest fire near la ronge sask canada       1  \n",
       "2  resident ask shelter place notify officer evac...       1  \n",
       "3    people receive wildfire evacuation order cal...       1  \n",
       "4  send photo ruby alaska smoke wildfire pour school       1  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill in cleaned_text column\n",
    "\n",
    "for i in range(len(train)):\n",
    "    train.loc[i, \"cleaned_text\"] = preprocess(train.loc[i, \"text\"])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burn</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kill</th>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        frequency\n",
       "like          391\n",
       "fire          359\n",
       "amp           300\n",
       "s             289\n",
       "new           232\n",
       "news          198\n",
       "people        198\n",
       "burn          174\n",
       "video         174\n",
       "kill          174"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store all unique words in our corpus\n",
    "\n",
    "unique_words = {}\n",
    "\n",
    "for text in train[\"cleaned_text\"]:\n",
    "    for word in text.split():\n",
    "        if (word not in unique_words.keys()):\n",
    "            unique_words[word] = 1\n",
    "        else:\n",
    "            unique_words[word] += 1\n",
    "            \n",
    "unique_words = pd.DataFrame.from_dict(unique_words, \n",
    "                                      orient=\"index\", \n",
    "                                      columns=[\"frequency\"])\n",
    "\n",
    "unique_words.sort_values(by=[\"frequency\"], inplace=True, ascending=False)\n",
    "\n",
    "# for later use\n",
    "total_unique_words = unique_words\n",
    "\n",
    "unique_words.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Let's see a histogram of the word frequencies\n",
    "_ = plt.hist(unique_words['frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's zoom in on the least frequently appearing words\n",
    "_ = plt.hist(unique_words['frequency'], range = (1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currently</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wwii</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bet</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunday</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usually</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1223 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           frequency\n",
       "like             391\n",
       "fire             359\n",
       "amp              300\n",
       "s                289\n",
       "new              232\n",
       "...              ...\n",
       "currently         10\n",
       "wwii              10\n",
       "bet               10\n",
       "sunday            10\n",
       "usually           10\n",
       "\n",
       "[1223 rows x 1 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see, there are many words that only appear once \n",
    "# or not often enough to improve the model so we will remove them\n",
    "unique_words = unique_words[unique_words['frequency'] >= 10]\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used the sklearn CountVectorizer function instead for computational efficiency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer(max_features = unique_words.shape[0]).fit_transform(train[\"cleaned_text\"].values).toarray()\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train: (6090, 1223)\n",
      "y_train: (6090,)\n",
      "\n",
      "X_test: (1523, 1223)\n",
      "y_test: (1523,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = bag_of_words\n",
    "y = train[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "print(f\"\"\"\n",
    "X_train: {X_train.shape}\n",
    "y_train: {y_train.shape}\n",
    "\n",
    "X_test: {X_test.shape}\n",
    "y_test: {y_test.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "Now it's time to create the models. I will optimize for parameters wherever possible, but will minimize them as much as possible and set random state for replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model - lr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Model - dt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Model - svc\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes Model - mnb\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19171\n"
     ]
    }
   ],
   "source": [
    "print(len(total_unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most number of pre-processed words to appear in a tweet is 137\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_text_count = [len(text) for text in train[\"cleaned_text\"]]\n",
    "_ = plt.hist(cleaned_text_count)\n",
    "print(f\"The most number of pre-processed words to appear in a tweet is {max(cleaned_text_count)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about the final CountVectorizer-processed format of our input data:\n",
    "- we are representing every tweet as a vector of 0 or 1 by whether the word appears, and each \"column\" is a unique word\n",
    "- we removed the least frequent words because they won't help in identifying patterns and only increase the complexity of the computation\n",
    "- X_train: (6090, 1223) and X_test: (1523, 1223), meaning I've decided that there are 1223 important unique words in our corpus that help indicate whether a tweet is a disaster or not\n",
    "\n",
    "In real life, CNNs are often used for the task of text classification so long as the corpus is converted into some standardized vector of numbers. Without knowing this, I chose to use a convolutional neural networks because they are a popular go-to method for prediction problems, and they work well with image data-like input. Given that we applied the count vectorizer function on our data to produce a matrix of 0-1 values much like an image, a CNN is appropriate here. \n",
    "\n",
    "https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn\n",
    "This is a great resource to further understand step-by-step how and why CNNs are used for the task of text classification.\n",
    "\n",
    "These are the justifications for every layer I add in my convolutional neural network:\n",
    "\n",
    "> Embedding layer <br> <br>\n",
    "Rationale: Maps integer representation of words (in our case, Count Vectorized form) into vector representationsn whose distinct values can be further analyzed to find patterns; often this is set to 32, 50, 100, or more; I choose 100 in this case <br> <br>\n",
    "Parameters:\n",
    ">- input_dim = the max number of distinct words or values in each input vector\n",
    ">- output_dim = the amount of vectors to process\n",
    ">- input_length = the length of each row (or the overall amount of words in each vector)\n",
    "<br> <br>\n",
    "> Read more: https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "\n",
    "> Convolutional layer <br> <br>\n",
    "Rationale: Transform the inputs from the previous layer to detect specific features from the appearance of unique word combinations against each other that indicate a \"disaster\" or not <br> <br>\n",
    "Parameters:\n",
    ">- filters = the number of output filters or convolutions; defaults include 32 or 100; I choose 100 in this case\n",
    ">- kernel_size = length of each convolution; I chose a default 5\n",
    ">- activation = \"relu\" for a computationally cheaper activation\n",
    "<br> <br>\n",
    "> Read more: https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12\n",
    "\n",
    "> A flattening layer <br> <br>\n",
    "Rationale: to prepare the output from the convolutional layer for the Dense layer\n",
    "\n",
    "> Two dense layers <br> <br>\n",
    "Rationale: the first layer deepens the CNN to further identify similar patterns in the previous layers; the second is the final binary classifying node <br> <br>\n",
    "Parameters: \n",
    ">- activation = \"relu\" computationally cheaper for the larger layer, and \"sigmoid\" for the final layer to outputs a number between 0 to 1 as required for the classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 [==============================] - 67s 349ms/step - loss: 0.5676 - accuracy: 0.7105\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 68s 354ms/step - loss: 0.4065 - accuracy: 0.8166\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 63s 329ms/step - loss: 0.3399 - accuracy: 0.8539\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 67s 352ms/step - loss: 0.2815 - accuracy: 0.8791\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 63s 329ms/step - loss: 0.2234 - accuracy: 0.9079\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 64s 333ms/step - loss: 0.1705 - accuracy: 0.9366\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 58s 304ms/step - loss: 0.1352 - accuracy: 0.9491\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 60s 316ms/step - loss: 0.1140 - accuracy: 0.9581\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 58s 306ms/step - loss: 0.0985 - accuracy: 0.9619\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 62s 324ms/step - loss: 0.0947 - accuracy: 0.9626\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-037568c0d1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m           verbose=1)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Convolutional Neural Network Model - dnn\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "\n",
    "cnn.add(Embedding(input_dim=137, \n",
    "                    output_dim=100, \n",
    "                    input_length=1223))\n",
    "cnn.add(Conv1D(filters=100, \n",
    "                 kernel_size=5, \n",
    "                 activation='relu'))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(100, activation='relu'))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "cnn.compile(optimizer='adam', \n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model Evaluation\n",
    "This is a section left for Henry, but simply based on the provided metric, these are the performances of all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models for future reference\n",
    "model_list = [('Logistic Regression', lr),\n",
    "              ('Decision Tree', dt),\n",
    "              ('Support Vector Machine', svc),\n",
    "              ('Multinomial Naive Bayes Model', mnb),\n",
    "              ('Convolutional Neural Network', cnn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Logistic Regression\n",
      "    Train Score: 0.8566502463054187\n",
      "    Test Score: 0.7977675640183848\n",
      "    F1 Score: 0.7416107382550335\n",
      "    \n",
      "\n",
      "    Decision Tree\n",
      "    Train Score: 0.9745484400656814\n",
      "    Test Score: 0.7150361129349967\n",
      "    F1 Score: 0.6488673139158576\n",
      "    \n",
      "\n",
      "    Support Vector Machine\n",
      "    Train Score: 0.9082101806239737\n",
      "    Test Score: 0.7964543663821405\n",
      "    F1 Score: 0.7318339100346021\n",
      "    \n",
      "\n",
      "    Multinomial Naive Bayes Model\n",
      "    Train Score: 0.8154351395730706\n",
      "    Test Score: 0.7925147734734077\n",
      "    F1 Score: 0.7371048252911814\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "f1_scores = []\n",
    "for model in model_list[:4]:\n",
    "    train_score = model[1].score(X_train, y_train)\n",
    "    train_scores.append(train_score)\n",
    "\n",
    "    test_score = model[1].score(X_test, y_test)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "    y_pred = model[1].predict(X_test)\n",
    "    _f1_score = f1_score(y_test, y_pred)\n",
    "    f1_scores.append(_f1_score)\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    {model[0]}\n",
    "    Train Score: {train_score}\n",
    "    Test Score: {test_score}\n",
    "    F1 Score: {_f1_score}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (different library so must specify/compute values differently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 4s 89ms/step - loss: 1.0058 - accuracy: 0.7656\n"
     ]
    }
   ],
   "source": [
    "train_score = 0.9626\n",
    "test_score = cnn.evaluate(X_test, y_test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict_classes(X_test)\n",
    "y_pred = y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "_f1_score = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append values to corresponding lists\n",
    "train_scores.append(train_score)\n",
    "test_scores.append(test_score)\n",
    "f1_scores.append(_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Convolutional Neural Network\n",
      "    Train Score: 0.9626\n",
      "    Test Score: 0.7655942440032959\n",
      "    F1 Score: 0.7056883759274527\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "    {model_list[4][0]}\n",
    "    Train Score: {train_score}\n",
    "    Test Score: {test_score}\n",
    "    F1 Score: {_f1_score}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Score</th>\n",
       "      <th>Test Score</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support Vector Machine</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multinomial Naive Bayes Model</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Model  Train Score  Test Score  F1 Score\n",
       "0            Logistic Regression        0.857       0.798     0.742\n",
       "1                  Decision Tree        0.975       0.715     0.649\n",
       "2         Support Vector Machine        0.908       0.796     0.732\n",
       "3  Multinomial Naive Bayes Model        0.815       0.793     0.737\n",
       "4   Convolutional Neural Network        0.963       0.766     0.706"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [model[0] for model in model_list]\n",
    "final_scores = {'Model': model_names,\n",
    "               'Train Score': train_scores,\n",
    "               'Test Score': test_scores,\n",
    "               'F1 Score': f1_scores}\n",
    "\n",
    "final_scores = pd.DataFrame(final_scores)\n",
    "final_scores['Train Score'] = final_scores['Train Score'].round(3)\n",
    "final_scores['Test Score'] = final_scores['Test Score'].round(3)\n",
    "final_scores['F1 Score'] = final_scores['F1 Score'].round(3)\n",
    "\n",
    "final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the logistic regression does the best in terms of both test and f1 score.\n",
    "\n",
    "This is not surprising in the realm of Data Science. Just because it is conceptually simple to grasp and easy to implement, does not necessarily mean it performs the poorest compared to other algorithms. \n",
    "\n",
    "Then again, the test and f1 values for all the models range around 0.7, so it could be argued that further optimization of each model would, though timely, produce more numerically spread and thus more comparable results.\n",
    "\n",
    "Either way, this is an exploration for Henry to build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "Abdullah, M. (2020, January 01). Disaster Tweets Solution. Retrieved from https://www.kaggle.com/mohamedabdullah/disaster-tweets-solution\n",
    "\n",
    "Choubey, V. (2020, July 22). Text classification using CNN. Retrieved from https://medium.com/voice-tech-podcast/text-classification-using-cnn-9ade8155dfb9\n",
    "\n",
    "Malik, U. (2019). Python for NLP: Word Embeddings for Deep Learning in Keras. Retrieved from https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\n",
    "\n",
    "Pantola, P. (2018, June 14). Natural Language Processing: Text Data Vectorization. Retrieved from https://medium.com/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7\n",
    "\n",
    "Wang, S. (2018, April 13). Build a Text Classification Program: An NLP Tutorial. Retrieved from https://www.toptal.com/machine-learning/nlp-tutorial-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdsm",
   "language": "python",
   "name": "pdsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
