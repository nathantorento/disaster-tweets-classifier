{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Tweets Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appnope==0.1.0 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
      "Collecting blis==0.4.1\n",
      "  Using cached blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.0 MB)\n",
      "Collecting catalogue==1.0.0\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting certifi==2020.11.8\n",
      "  Using cached certifi-2020.11.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: chardet==3.0.4 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (3.0.4)\n",
      "Collecting cymem==2.0.4\n",
      "  Using cached cymem-2.0.4-cp37-cp37m-macosx_10_9_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: decorator==4.4.2 in /opt/anaconda3/envs/myenv/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (4.4.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement en-core-web-sm==2.3.1 (from -r requirements.txt (line 9)) (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for en-core-web-sm==2.3.1 (from -r requirements.txt (line 9))\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing all dependencies required for the notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv', index_col='id')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # 7613 rows, with 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target 1 refers to disaster tweet, 0 is not a disaster tweet\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 rows have no keywords\n",
      "2533 rows have no location\n",
      "0 rows have no text\n",
      "0 rows have no target\n"
     ]
    }
   ],
   "source": [
    "# checking for completeness of data\n",
    "print(f\"{np.sum(df['keyword'].isna())} rows have no keywords\")\n",
    "print(f\"{np.sum(df['location'].isna())} rows have no location\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no text\")\n",
    "print(f\"{np.sum(df['text'].isna())} rows have no target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fatalities               45\n",
       "armageddon               42\n",
       "deluge                   42\n",
       "sinking                  41\n",
       "body%20bags              41\n",
       "                         ..\n",
       "forest%20fire            19\n",
       "epicentre                12\n",
       "threat                   11\n",
       "inundation               10\n",
       "radiation%20emergency     9\n",
       "Name: keyword, Length: 221, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that some keywords are phrases, with '%20' as a space\n",
    "df['keyword'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                  104\n",
       "New York              71\n",
       "United States         50\n",
       "London                45\n",
       "Canada                29\n",
       "                    ... \n",
       "a box                  1\n",
       "Las Vegas, NV          1\n",
       "MD                     1\n",
       "Bangalore, INDIA       1\n",
       "Highland Park, CA      1\n",
       "Name: location, Length: 3341, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that there are some non-location locations, like 'World Wide!!' and 'a feminist, modernist hag.'\n",
    "df['location'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in ./venv/lib/python3.9/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in ./venv/lib/python3.9/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.52.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: thinc==7.4.1 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.9/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Users/weiting/Code/disaster-tweets-classifier/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download spaCy model for American English\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying spaCy's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "2020\t\t2020\t\tFalse\n",
      "ca\t\tcan\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "get\t\tget\t\tTrue\n",
      "any\t\tany\t\tTrue\n",
      "worse\t\tbad\t\tFalse\n",
      "#\t\t#\t\tFalse\n",
      "ihate2020\t\tihate2020\t\tFalse\n",
      "@bestfriend\t\t@bestfriend\t\tFalse\n",
      "https://t.co\t\thttps://t.co\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Let's see what spaCy does with numbers, contractions, #hashtags, @mentions and URLs\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend https://t.co\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "2020\t\t2020\t\tFalse\n",
      "ca\t\tcan\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "get\t\tget\t\tTrue\n",
      "any\t\tany\t\tTrue\n",
      "worse\t\tbad\t\tFalse\n",
      "#ihate2020\t\t#ihate2020\t\tFalse\n",
      "@bestfriend\t\t@bestfriend\t\tFalse\n",
      "https://t.co\t\thttps://t.co\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Contractions are split into lemmas\n",
    "# Numbers are their own features\n",
    "# @mentions are maintained as a token\n",
    "# We want to also keep #hashtags as a token, so we will modify the spaCy model's token_match\n",
    "\n",
    "import re \n",
    "\n",
    "# Retrieve the default token-matching regex pattern\n",
    "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n",
    "\n",
    "# Add #hashtag pattern\n",
    "re_token_match = f\"({re_token_match}|#\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Now let's try again\n",
    "s = \"2020 can't get any worse #ihate2020 @bestfriend https://t.co\"\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Token\t\tLemma\t\tStopword\n",
      "========================================\n",
      "our\t\t-PRON-\t\tTrue\n",
      "deeds\t\tdeed\t\tFalse\n",
      "are\t\tbe\t\tTrue\n",
      "the\t\tthe\t\tTrue\n",
      "reason\t\treason\t\tFalse\n",
      "of\t\tof\t\tTrue\n",
      "this\t\tthis\t\tTrue\n",
      "#earthquake\t\t#earthquake\t\tFalse\n",
      "may\t\tmay\t\tTrue\n",
      "allah\t\tallah\t\tFalse\n",
      "forgive\t\tforgive\t\tFalse\n",
      "us\t\t-PRON-\t\tTrue\n",
      "all\t\tall\t\tTrue\n",
      "@mention\t\t@mention\t\tFalse\n",
      "#hashtag\t\t#hashtag\t\tFalse\n",
      "http://t.co/test\t\thttp://t.co/test\t\tFalse\n",
      "<class 'str'>\n",
      "Bag of words for the tweet: {'#': 2, '@': 1, 'URL': 1, '-PRON-': 2, 'deed': 1, 'be': 1, 'the': 1, 'reason': 1, 'of': 1, 'this': 1, '#earthquake': 1, 'may': 1, 'allah': 1, 'forgive': 1, 'all': 1, '@mention': 1, '#hashtag': 1, 'http://t.co/test': 1}\n"
     ]
    }
   ],
   "source": [
    "# Features is a set of all lemmas (words) encountered thus far, add hashtags, mentions and URLs to track the number of each respectively\n",
    "features = set({'#','@','URL'})\n",
    "\n",
    "# Now let's process an original tweet with our modified spaCy model\n",
    "s = df.loc[1,'text']\n",
    "print(f\"Original tweet: {s}\")\n",
    "\n",
    "# Modifying the tweet to include mentions, hashtags and a URL\n",
    "s += ' @mention #hashtag http://t.co/test'\n",
    "\n",
    "# To lowercase\n",
    "s = s.lower()\n",
    "\n",
    "# Creating a doc with spaCy\n",
    "doc = nlp(s)\n",
    "\n",
    "# Let's look at the lemmas and is stopword of each token\n",
    "print(f\"Token\\t\\tLemma\\t\\tStopword\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "lemmas = []\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "    lemmas.append(token.lemma_)\n",
    "\n",
    "# Union between lemmas and our features set\n",
    "features |= set(lemmas)\n",
    "\n",
    "# Constructing a bag of words for the tweet\n",
    "freq = {'#':0,'@':0,'URL':0}\n",
    "for word in lemmas:\n",
    "    freq[str(word)] = 0\n",
    "for token in doc: \n",
    "    if '#' in str(token): freq['#'] += 1 # Count number of hashtags, regardless of hashtag\n",
    "    if '@' in str(token): freq['@'] += 1 # Count number of mentions, regardless of mention\n",
    "    if 'http://' in str(token): freq['URL'] += 1 # Count number of URLs, regardless of URL\n",
    "    freq[str(token.lemma_)] += 1\n",
    "print(type(str(token)))\n",
    "print(f\"Bag of words for the tweet: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've preprocessed a single tweet, we can create a pre-process function for each tweet\n",
    "def preprocess(s, nlp, features):\n",
    "    \"\"\"\n",
    "    Given string s, spaCy model nlp, and set features (lemmas encountered),\n",
    "    pre-process s and return updated features and bag-of-words representation dict freq\n",
    "    - changes s to lower-case\n",
    "    - tokenize s using nlp to create a doc\n",
    "    - update features with lemmas encountered in s\n",
    "    - create bag-of-words representation in dict type freq, including counts for hashtags, mentions and URLs\n",
    "    \"\"\"\n",
    "\n",
    "    # To lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Creating a doc with spaCy\n",
    "    doc = nlp(s)\n",
    "\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        lemmas.append(token.lemma_)\n",
    "\n",
    "    # Union between lemmas and our features set\n",
    "    features |= set(lemmas)\n",
    "\n",
    "    # Constructing a bag of words for the tweet\n",
    "    freq = {'#':0,'@':0,'URL':0}\n",
    "    for word in lemmas:\n",
    "        freq[str(word)] = 0\n",
    "    for token in doc: \n",
    "        if '#' in str(token): freq['#'] += 1 # Count number of hashtags, regardless of hashtag\n",
    "        if '@' in str(token): freq['@'] += 1 # Count number of mentions, regardless of mention\n",
    "        if 'http://' in str(token): freq['URL'] += 1 # Count number of URLs, regardless of URL\n",
    "        freq[str(token.lemma_)] += 1\n",
    "        \n",
    "    return features, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = df # Duplicate for preprocessing\n",
    "features = set({'#','@','URL'}) # Using set feature to contain all words (lemmas) seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 0,\n",
       "  '-PRON-': 2,\n",
       "  'well': 2,\n",
       "  'than': 2,\n",
       "  'sean': 3,\n",
       "  'bro': 1,\n",
       "  '.': 2,\n",
       "  'i': 1,\n",
       "  'can': 1,\n",
       "  'admit': 1,\n",
       "  'that': 1,\n",
       "  'be': 1,\n",
       "  'flame': 1,\n",
       "  'now': 1,\n",
       "  'but': 1,\n",
       "  'https://t.co/aomq1rykmj': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'eu': 1,\n",
       "  'states': 1,\n",
       "  'squabble': 1,\n",
       "  'over': 1,\n",
       "  'immigration': 1,\n",
       "  '.': 2,\n",
       "  'uk': 1,\n",
       "  '-': 1,\n",
       "  'france': 1,\n",
       "  'eurotunnel': 1,\n",
       "  'deluge': 1,\n",
       "  'with': 1,\n",
       "  'migrant': 1,\n",
       "  'one': 1,\n",
       "  'dead': 1,\n",
       "  'as': 1,\n",
       "  \"'\": 2,\n",
       "  'thousand': 1,\n",
       "  'storm': 1,\n",
       "  'tunnel': 1,\n",
       "  'http://t.co/vf6cklmcsx': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'photoset': 1,\n",
       "  ':': 4,\n",
       "  'littlebitofbass': 1,\n",
       "  'silinski': 1,\n",
       "  'ed': 1,\n",
       "  'sheeran': 1,\n",
       "  \"onåê'the\": 1,\n",
       "  'hobbit': 1,\n",
       "  'the': 1,\n",
       "  'desolation': 1,\n",
       "  'of': 1,\n",
       "  'smaug': 1,\n",
       "  \"'\": 1,\n",
       "  'german': 1,\n",
       "  'premiere': 1,\n",
       "  '...': 1,\n",
       "  'http://t.co/iosthxlcyv': 1},\n",
       " {'#': 0, '@': 0, 'URL': 0, 'armageddon': 1, 'https://t.co/ucsudk3q1d': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'correction': 1,\n",
       "  ':': 1,\n",
       "  'tent': 1,\n",
       "  'collapse': 1,\n",
       "  'story': 1,\n",
       "  'http://t.co/s7vygenjuv': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 2,\n",
       "  'infantry': 1,\n",
       "  'mens': 1,\n",
       "  'lume': 1,\n",
       "  'dial': 1,\n",
       "  'army': 1,\n",
       "  'analog': 1,\n",
       "  'quartz': 1,\n",
       "  'wrist': 1,\n",
       "  'watch': 1,\n",
       "  'sport': 1,\n",
       "  'blue': 1,\n",
       "  'nylon': 1,\n",
       "  'fabric': 1,\n",
       "  ' ': 1,\n",
       "  '-': 1,\n",
       "  'full': 1,\n",
       "  'rea\\x89û': 1,\n",
       "  '_': 1,\n",
       "  'http://t.co/hep9k0xghb': 1,\n",
       "  'http://t.co/80ebvglmra': 1},\n",
       " {'#': 2,\n",
       "  '@': 3,\n",
       "  'URL': 0,\n",
       "  '@carsonrex': 1,\n",
       "  '@spaceangelseven': 1,\n",
       "  'check': 1,\n",
       "  'out': 1,\n",
       "  'this': 1,\n",
       "  '#rockin': 1,\n",
       "  'preview': 1,\n",
       "  'of': 1,\n",
       "  '@claytonbryant': 1,\n",
       "  'danger': 1,\n",
       "  'zone': 1,\n",
       "  'come': 1,\n",
       "  'soon': 1,\n",
       "  '!': 1,\n",
       "  'https://t.co/p0fizxmn5r': 1,\n",
       "  '#artistsunite': 1},\n",
       " {'#': 1,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  \"'\": 2,\n",
       "  'when': 1,\n",
       "  '-PRON-': 2,\n",
       "  'attack': 2,\n",
       "  'woman': 1,\n",
       "  \"'s\": 2,\n",
       "  'health': 2,\n",
       "  'america': 1,\n",
       "  '.': 1,\n",
       "  'hillary': 1,\n",
       "  'clinton': 1,\n",
       "  'show': 1,\n",
       "  'how': 1,\n",
       "  'to': 1,\n",
       "  '#standwithpp': 1,\n",
       "  'http://t.co/hxdg254dho': 1},\n",
       " {'#': 0,\n",
       "  '@': 1,\n",
       "  'URL': 0,\n",
       "  '@haley_whaley': 1,\n",
       "  'hailstorm': 1,\n",
       "  'hey': 1,\n",
       "  'there': 1,\n",
       "  'be': 1,\n",
       "  'a': 1,\n",
       "  'secret': 1,\n",
       "  'trick': 1,\n",
       "  'to': 1,\n",
       "  'get': 1,\n",
       "  '375.000': 1,\n",
       "  'gem': 1,\n",
       "  'clash': 1,\n",
       "  'ofclan': 1,\n",
       "  'check': 1,\n",
       "  '-PRON-': 2,\n",
       "  'now': 1,\n",
       "  'on': 1,\n",
       "  'profile': 1},\n",
       " {'#': 0,\n",
       "  '@': 5,\n",
       "  'URL': 0,\n",
       "  '@creationmin': 1,\n",
       "  '@rwrabbit': 1,\n",
       "  '@gooneratheist': 1,\n",
       "  '@atheistic_1': 1,\n",
       "  '@lolatjesus': 1,\n",
       "  'yet': 1,\n",
       "  'still': 1,\n",
       "  'why': 1,\n",
       "  'do': 1,\n",
       "  'so': 1,\n",
       "  'many': 1,\n",
       "  'fish': 1,\n",
       "  'die': 1,\n",
       "  'in': 1,\n",
       "  'the': 1,\n",
       "  'worldwide': 1,\n",
       "  'flood': 1,\n",
       "  '?': 1,\n",
       "  'i': 1,\n",
       "  'wonder': 1,\n",
       "  '.': 1},\n",
       " {'#': 5,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'day': 1,\n",
       "  '2': 1,\n",
       "  '.': 2,\n",
       "  'liquidation': 1,\n",
       "  'of': 1,\n",
       "  'emergency': 1,\n",
       "  'at': 1,\n",
       "  'chemical': 1,\n",
       "  'object': 1,\n",
       "  '#usar2015': 1,\n",
       "  '#usar15': 1,\n",
       "  '#ruor': 1,\n",
       "  '?': 11,\n",
       "  'http://t.co/ggtmdquddo': 1},\n",
       " {'#': 1,\n",
       "  '@': 1,\n",
       "  'URL': 0,\n",
       "  '@mikeparractor': 1,\n",
       "  'have': 1,\n",
       "  'confirm': 1,\n",
       "  'on': 1,\n",
       "  '-PRON-': 1,\n",
       "  'twitter': 1,\n",
       "  'say': 1,\n",
       "  'goodbye': 1,\n",
       "  '2': 1,\n",
       "  'ross': 1,\n",
       "  '.': 1,\n",
       "  'be': 1,\n",
       "  'bloody': 1,\n",
       "  'gobsmacked': 1,\n",
       "  '/': 1,\n",
       "  'devastate': 1,\n",
       "  '#emmerdale': 1},\n",
       " {'#': 3,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'only': 1,\n",
       "  'one': 1,\n",
       "  'man': 1,\n",
       "  'tsutomi': 1,\n",
       "  'yamaguchi': 1,\n",
       "  'be': 1,\n",
       "  'say': 1,\n",
       "  'to': 1,\n",
       "  'have': 1,\n",
       "  'survive': 1,\n",
       "  'both': 1,\n",
       "  'atomic': 1,\n",
       "  'bomb': 1,\n",
       "  'blast': 1,\n",
       "  'at': 1,\n",
       "  '#hiroshima': 1,\n",
       "  'and': 1,\n",
       "  '#nagasaki.': 1,\n",
       "  '#otd': 1,\n",
       "  'http://t.co/daalpenzp0': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'george': 1,\n",
       "  'njenga': 1,\n",
       "  'the': 1,\n",
       "  'hero': 1,\n",
       "  'save': 1,\n",
       "  '-PRON-': 1,\n",
       "  'burn': 1,\n",
       "  'friend': 1,\n",
       "  'from': 1,\n",
       "  'a': 1,\n",
       "  'razing': 1,\n",
       "  'wildfire': 1,\n",
       "  '...': 1,\n",
       "  'http://t.co/us8r6qsn0p': 1},\n",
       " {'#': 1,\n",
       "  '@': 0,\n",
       "  'URL': 2,\n",
       "  'escape': 1,\n",
       "  'the': 2,\n",
       "  'heat': 1,\n",
       "  '(': 1,\n",
       "  'and': 1,\n",
       "  '#orshow)': 1,\n",
       "  'for': 1,\n",
       "  'a': 1,\n",
       "  'trail': 1,\n",
       "  'run': 1,\n",
       "  'on': 1,\n",
       "  'desolation': 1,\n",
       "  'loop': 1,\n",
       "  '-PRON-': 2,\n",
       "  'will': 1,\n",
       "  'be': 1,\n",
       "  'glad': 1,\n",
       "  'do': 1,\n",
       "  'http://t.co/n2ucnzh38p': 1,\n",
       "  'http://t.co/vu8fwymw5r': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'fear': 1,\n",
       "  'and': 3,\n",
       "  'panic': 1,\n",
       "  'in': 1,\n",
       "  'the': 1,\n",
       "  'air': 1,\n",
       "  '\\n': 3,\n",
       "  'i': 3,\n",
       "  'want': 1,\n",
       "  'to': 1,\n",
       "  'be': 1,\n",
       "  'free': 1,\n",
       "  'from': 1,\n",
       "  'desolation': 1,\n",
       "  'despair': 1,\n",
       "  'feel': 1,\n",
       "  'like': 1,\n",
       "  'everything': 1,\n",
       "  'sow': 1,\n",
       "  '?': 1,\n",
       "  'http://t.co/ixw2cutk1c': 1},\n",
       " {'#': 0,\n",
       "  '@': 3,\n",
       "  'URL': 0,\n",
       "  '\\x89ûï@ymcglaun': 1,\n",
       "  ':': 1,\n",
       "  '@juliekragt': 1,\n",
       "  '@wildwestsixgun': 1,\n",
       "  '-PRON-': 1,\n",
       "  'be': 1,\n",
       "  'a': 3,\n",
       "  'lot': 2,\n",
       "  'safe': 1,\n",
       "  'that': 1,\n",
       "  'way.\\x89û\\x9dyeah': 1,\n",
       "  'more': 1,\n",
       "  'stable': 1,\n",
       "  '&': 1,\n",
       "  'amp': 1,\n",
       "  ';': 1,\n",
       "  'if': 1,\n",
       "  'i': 2,\n",
       "  'get': 1,\n",
       "  'in': 1,\n",
       "  'trouble': 1,\n",
       "  'have': 1,\n",
       "  'seat': 1,\n",
       "  'right': 1,\n",
       "  'there': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 0,\n",
       "  'look': 1,\n",
       "  'like': 1,\n",
       "  'a': 1,\n",
       "  'war': 1,\n",
       "  'zone': 1,\n",
       "  'outside': 1,\n",
       "  '.': 1,\n",
       "  'what': 1,\n",
       "  'be': 1,\n",
       "  'go': 1,\n",
       "  'on': 1,\n",
       "  '?': 1},\n",
       " {'#': 0,\n",
       "  '@': 1,\n",
       "  'URL': 0,\n",
       "  '@daborsch': 1,\n",
       "  'not': 1,\n",
       "  'really': 1,\n",
       "  'that': 1,\n",
       "  'shocking': 1,\n",
       "  ':(': 1,\n",
       "  'blizzard': 1,\n",
       "  'lure': 1,\n",
       "  '-PRON-': 2,\n",
       "  'old': 1,\n",
       "  'fanbase': 1,\n",
       "  'back': 1,\n",
       "  'with': 1,\n",
       "  'wod': 1,\n",
       "  'and': 1,\n",
       "  'disappoint': 1,\n",
       "  'hardcore': 1,\n",
       "  'so': 1,\n",
       "  'everyone': 1,\n",
       "  'leave': 1,\n",
       "  'again': 1},\n",
       " {'#': 0,\n",
       "  '@': 0,\n",
       "  'URL': 1,\n",
       "  'natural': 2,\n",
       "  'disaster': 2,\n",
       "  '\\x89ûò': 1,\n",
       "  'news': 1,\n",
       "  'story': 1,\n",
       "  'about': 1,\n",
       "  '-': 1,\n",
       "  'page': 1,\n",
       "  '1': 1,\n",
       "  '|': 1,\n",
       "  'newser': 1,\n",
       "  'http://t.co/tb8gzembxu': 1}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleh = []\n",
    "for i in range(len(foo)):\n",
    "    features, freq = preprocess(foo.iloc[i]['text'], nlp, features)\n",
    "    bleh.append(freq)\n",
    "bleh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-8b631c68288a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbow_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Array bow_array of bow representations for each tweet; bow_array[i] is the bow representation for tweet id (i+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbow_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-d633207a6a34>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(s, nlp, features)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Creating a doc with spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36muniqued_fwd\u001b[0;34m(X, drop)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         uniq_keys, ind, inv, counts = numpy.unique(\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         )\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mimask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0minv_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0minv_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m     \"\"\"\n\u001b[0;32m-> 2483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cumsum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pdsm/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bow_array = [] # Array bow_array of bow representations for each tweet; bow_array[i] is the bow representation for tweet id (i+1)\n",
    "for i in range(len(preprocess_df)):\n",
    "    features, freq = preprocess(preprocess_df.iloc[i]['text'],nlp,features)\n",
    "    bow_array.append(freq)\n",
    "len(bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_df.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for bag of words representation for each tweet\n",
    "bow = pd.DataFrame('0', columns=features,index=range(1,len(preprocess_df)+1))\n",
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update bow[i] with bag-of-words freq of the tweet id (i+1)\n",
    "for i in range(len(preprocess_df)):\n",
    "    freq = bow_array[i]\n",
    "    for f in freq:\n",
    "        bow.loc[i+1,f]=freq[f]\n",
    "\n",
    "# Join bag-of-words representation to train dataframe\n",
    "# Append _data suffix to 'keyword','location','text','target' for features that are not lemma tokens\n",
    "preprocess_df = preprocess_df.join(bow,lsuffix='_data')\n",
    "\n",
    "# Saving bag-of-words representation for collaborators\n",
    "preprocess_df.to_csv(\"data/train_preprocessed.csv\",index=True,index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 21330) (762, 21330)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# stratify=y creates a balanced validation set\n",
    "y = preprocess_df['target_data']\n",
    "\n",
    "df_train, df_val = train_test_split(preprocess_df, test_size=0.10, random_state=101, stratify=y)\n",
    "\n",
    "# Saving csv files for collaborators\n",
    "df_train.to_csv(\"data/train_preprocessed_split.csv\",index=True)\n",
    "df_val.to_csv(\"data/val_preprocessed_split.csv\",index=True)\n",
    "\n",
    "print(df_train.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ratio of target=1 to target=0 tweets in:\n",
      " \n",
      "Original data set = 0.7533394748963611,\n",
      "\n",
      "Training data set = 0.7535193242897363,\n",
      "\n",
      "Validation data set = 0.7517241379310344\n"
     ]
    }
   ],
   "source": [
    "# Checking balance\n",
    "print(f\"\"\"\n",
    "Ratio of target=1 to target=0 tweets in:\\n \n",
    "Original data set = {np.sum(preprocess_df['target_data']==1)/np.sum(preprocess_df['target_data']==0)},\\n\n",
    "Training data set = {np.sum(df_train['target_data']==1)/np.sum(df_train['target_data']==0)},\\n\n",
    "Validation data set = {np.sum(df_val['target_data']==1)/np.sum(df_val['target_data']==0)}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdsm",
   "language": "python",
   "name": "pdsm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
